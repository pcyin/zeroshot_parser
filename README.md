# On the Ingredients of an Effective Zero-shot Semantic Parser

This repo contains the implementation of our [ACL 2022 paper](TBD) *On the Ingredients of an Effective Zero-shot Semantic Parser*.
It is a zero-shot semantic parsing system on the Scholar and GEO datasets.

## Setup

**Conda Environment** Install the conda Python environment

```shell
conda create -f data/env.yml
```

**Data Files** Download data artifacts from [here](TBD) and unzip its contents.
List of data files:

* Paraphraser: `paraphrase_model.tar.gz`
* Canonical examples generated from the SCFG: `canonical_examples.tar.gz`

**Evaluation Server** Set up the SEMPRE evaluation server to execute $\lambda$-calculus 
logical forms generated by a model for functional correctness evaluation.
The evaluator is based on the one used in Herzig and Berant (2019), with
improvements to host the evaluator as an HTTP server, an additional 
caching layer to avoid duplicate execution, and to allow parallel 
execution. This significantly improves speed on GEO and Scholar. On a 20-thread
server evaluation typically takes 700s once the cache is warmed up. 
Code for the executor is at [here](https://github.com/pcyin/sempre/tree/executor). 

```shell
# Untar new_evaluator/sempre/third_party.tar.gz before starting the server.
cd new_evaluator/sempre
tar -zxvf third_party.tar.gz

cd ../../
./new_evaluator/start_server_[geo|scholar].sh
```

You may need to change the default ports (8081 for Scholar and 8082 for GEO) 
as appropriate. Note that the evaluation server uses caching, which may consume a lot of
memory. In our experiment the evaulation server is hosted on a machine with 128GB RAM.

Once you spin up the evaluation server, add it address to `exp/run.py`. See
`run.py` for details.

## Usage

### Training

Training consists of two steps: (1) a bootstrap step to generate
validation data, and (2) the actual training step. Model training
also depends on the following artifacts:

* A paraphrase generate model (`BART Large`)
* A paraphrase identification model [`SIM`](https://github.com/jwieting/simple-and-effective-paraphrastic-similarity)
(nNot used in the paper ).
* Generated canonical utterances.
* Real evaluation data (development and test splits. Only used for reporting).

You may refer to other sections for details of generating 
those artifacts. They are also included in the data release.

`exp/run.py` is the script to launch experiments. See its code for details.
We run our experiments using a similar launch script on a slurm 
cluster. Please contact the first author if you need these scripts.

```shell
python -m exp.run
```


### Generate Canonical Samples from SCFG.

Our grammar is based on the ones used in Herzig and Berant (2019), with
additional idiomatic productions to capture domain-specific language patterns.

The data artifacts associated with this release already include 
canonical sampled generated from the grammar and scored by GPT-2.
To reproduce this process, use the following script:

```bash
sh grammar_generation/scripts/generate_canonical_examples.sh
```

By default the script will generate all canonical examples up to 
a max logical form depth of 6 and score them using GPT-2 XL. 
The output is fairly large, and may take several hours to finish.

After all the canonical examples are scored, use `exp/generate_data.py`
to select a subset of examples for training (Section 3.1.2, Naturalness-driven 
Data Selection).
The script will also generate different sub-samples used in the ablation
study of the paper.

## Reference

```
@inproceedings{yin22acl,
    title = {On The Ingredients of an Effective Zero-shot Semantic Parser},
    author = {Pengcheng Yin and John Frederick Wieting and Avirup Sil and Graham Neubig},
    booktitle = {Annual Conference of the Association for Computational Linguistics (ACL)},
    address = {Dublin, Ireland},
    month = {May},
    url = {https://arxiv.org/abs/2110.08381},
    year = {2022}
}
```